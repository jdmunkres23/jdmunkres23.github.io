---
layout: single
categories: blog
title: "혼공머신 ch4-1"
use_math: true
---

# chapter 4-1 로지스틱 회귀 - 실습

## 0. 소개

이번 실습에서는 물고리의 특성을 이용해서 분류하지만, 각각의 물고기일 확률을 제공해주는 분류 알고리즘을 만들어본다. 가장 먼저, 이전에 배웠던 k-최근접 이웃 알고리즘을 이용해서 이웃의 클래스 비율을 확률로 나타내는 다중 분류 모델을 만들어본다. 그리고 이 알고리즘의 한계점을 살펴보고 이를 극복할 새로운 분류 알고리즘인 로지스틱 회귀 알고리즘을 배워보고 이를 이용해서 이진 분류와 다중 분류 문제를 해결해본다. 정리하면 다음과 같다.

1. k-최근접 이웃 분류기로 확률 예측하기(다중 분류)
2. 로지스틱 회귀 알고리즘 배우기
3. 이진 분류와 다중 분류 문제에서 클래스 확률 예측하기
.

## 1. K-최근접 이웃

지금까지 분류와 회귀에 대해서 배웠다. 이번에 주어진 문제는 생선의 5가지 특성(무게, 길이, 대각선, 높이, 두께)을 통해서 7개의 생선 중 어떤 생선일지 확률을 예측하는 분류 알고리즘을 만들어보자.

먼저, 이전에 배웠던 k-최근접 이웃 알고리즘을 이용해서 이웃한 클래스의 비율을 확률로 주는 모델을 만들어보자.


### Step1. 데이터 준비하기




```python
# Load data
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Weight</th>
      <th>Length</th>
      <th>Diagonal</th>
      <th>Height</th>
      <th>Width</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Bream</td>
      <td>242.0</td>
      <td>25.4</td>
      <td>30.0</td>
      <td>11.5200</td>
      <td>4.0200</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Bream</td>
      <td>290.0</td>
      <td>26.3</td>
      <td>31.2</td>
      <td>12.4800</td>
      <td>4.3056</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Bream</td>
      <td>340.0</td>
      <td>26.5</td>
      <td>31.1</td>
      <td>12.3778</td>
      <td>4.6961</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Bream</td>
      <td>363.0</td>
      <td>29.0</td>
      <td>33.5</td>
      <td>12.7300</td>
      <td>4.4555</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Bream</td>
      <td>430.0</td>
      <td>29.0</td>
      <td>34.0</td>
      <td>12.4440</td>
      <td>5.1340</td>
    </tr>
  </tbody>
</table>
</div>



> **데이터프레임(dataframe)** <br>
위의 코드 실행 결과인 표는 데이터프레임(dataframe)으로 판다스에서 제공하는 2차원 표 형식의 주요 데이터 구조이다. 데이터프레임의 특징은 다음과 같다.
>1. 넘파이 배열과 비슷하게 열과 행으로 이루어져있다.
>2. 통계와 그래프를 위한 메서드를 풍부하게 제공한다.
>3. 넘파이로 상호 변환이 쉽고 사이킷런과 잘 호환된다.

unique() 함수를 이용해서 데이터프레임의 Species열에 고유한 값을 추출할 수 있다. 이를 이용해서 어떤 종류의 생선이 있는지 살펴보자.


```python
print(pd.unique(fish['Species']))
```

    ['Bream' 'Roach' 'Whitefish' 'Parkki' 'Perch' 'Pike' 'Smelt']
    

우리가 맞추고 싶은 생선의 갯수 즉, 타깃 데이터는 7종류이다. 이처럼 2개 이상의 클래스가 포함된 문제를 **다중 분류(multi-class classification)**이라고 한다.

fish 데이터프레임에서 Species 열을 target으로 하고 Weight부터 Width까지 열을 feature로 사용하자.

데이터프레임에서 열을 선택하는 방법은 원하는 열을 리스트로 나열하는 것이다.
단, 열을 하나만 선택할 때는 리스트가 아닌 문자열 하나만 입력한다.  
*리스트로 전달하면 2차원 배열이 된다.


```python
# extract feature columns
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
print(fish_input[:5])
```

    [[242.      25.4     30.      11.52     4.02  ]
     [290.      26.3     31.2     12.48     4.3056]
     [340.      26.5     31.1     12.3778   4.6961]
     [363.      29.      33.5     12.73     4.4555]
     [430.      29.      34.      12.444    5.134 ]]
    


```python
# extract target column
fish_target = fish['Species'].to_numpy()
```

#### 데이터 분리하기


```python
# Split data to train and test
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)
```

#### 스케일링


```python
# scaling
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

## Step2. 모델 훈련 및 평가


```python
# train model 
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)

# evaluation
print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))
```

    0.8907563025210085
    0.85
    

## Step3. 결과 해석

앞서 설명했듯이 지금은 다중 분류를 하고 있지만, 2장에서 만들었던 이진 분류와 모델을 만들고 훈련하는 방식은 동일한 것을 볼 수 있다.

이진 분류에서는 양성 클래스와 음성 클래스를 1과 0으로 지정하여 타깃 데이터를 만들었다. 다중 분류에서도 숫자로 바꾸어 입력할 수 있지만, 사이킷런에서는 편리하게 문자열로 된 타깃값을 그대로 사용할 수 있다.

### classes_ 속성

**여기서 주의할 점은 타깃값을 그대로 사이킷런 모델에 전달하면 순서가 자동으로 알파벳 순으로 매겨진다는 것이다.** 따라서, 처음에 unique() 함수로 확인했던 Species열의 고유한 값들의 순서와 타깃 데이터의 순서는 다르다. KNeighborsClassifier에서 정렬된 타깃값은 classes_ 속성에 저장되어 있다. 이를 확인해보자.


```python
# order of target data
print(kn.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    

위의 순서가 클래스의 순서이다. 즉, 'Bream'이 첫 번째 클래스, 'Parkki'가 두 번재 클래스가 되는 식이다.

### predict() 메서드

타깃값으로 예측을 출력해주는 predict() 메서드를 이용해서 테스트 세트에 있는 처음 5개 샘플의 타깃값을 예측해보자.


```python
print(kn.predict(test_scaled[:5]))
```

    ['Perch' 'Smelt' 'Pike' 'Perch' 'Perch']
    

### predict_proba() 메서드

예측된 5개의 타깃값은 어떤 확률로 만들어졌을까? 사이킷런의 분류 모델은 predict_proba()메서드로 클래스별 확률값을 반환한다. 이를 이용해서 위의 5개의 샘플에 대한 확률을 출력해보자.

* 넘파이 round() 함수 : 기본으로 소수점 첫째 자리에서 반올림, decimals 매개변수로 유지할 소수점 아래 자릿수를 지정할 수 있다.


```python
# Check Probability value
import numpy as np
proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))
```

    [[0.     0.     1.     0.     0.     0.     0.    ]
     [0.     0.     0.     0.     0.     1.     0.    ]
     [0.     0.     0.     1.     0.     0.     0.    ]
     [0.     0.     0.6667 0.     0.3333 0.     0.    ]
     [0.     0.     0.6667 0.     0.3333 0.     0.    ]]
    

predict_proba() 메서드의 출력 순서는 앞서 보았던 classes_ 속성과 같다. 즉, 앞에서부터 첫 번째 열은 'Bream'에 대한 확률, 두 번째 열은 'Parkki'에 대한 확률, 이렇게 일곱 번째 열은 일곱 번째 클래스에 대한 확률이다. 

이 모델이 계산한 확률이 가장 가까운 이웃의 비율이 맞는지 확인해보자. 네 번째 샘플에 대해서만 확인해보자.

* kneighbors() 메서드의 입력은 2차원 배열이어야 한다. 이를 위해 넘파이 배열의 슬라이싱 연산자를 사용했다. 슬라이싱 연산자는 하나의 샘플만 선택해도 항상 2차원 배열이 만들어진다.


```python
distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])
```

    [['Roach' 'Perch' 'Perch']]
    

다섯 번째의 클래스인 'Roach'가 1개이고 세 번째 클래스인 'Perch'가 2개 이므로 다섯 번째 클래스에 대한 확률은 0.3333..이고 세 번째 클래스에 대한 확률은 0.6666..이다. 아주 잘 예측을 해냈음을 볼 수 있다. 

그러나 k-최근접 이웃은 확률이 0, 1/3, 2/3, 3/3 뿐이다. 즉, 클래스가 $n$개일 때, 가질 수 있는 확률은 $k/n, (k=1,2,...,n)$형태로 매우 제한적이다. 조금 더 확률적인 의미를 가질 수 있는 분류 모델이 필요하다.


# 2. 로지스틱 회귀

로지스틱 회귀는 선형 방정식을 학습한다. 이중 분류에서는 시그모이드 함수를 이용해 출력값을 0~1 사이의 확률로 압축시키고 다중 분류에서는 소프트맥스 함수를 이용해서 0~1 사이의 확률로 압축시킨다. 이런 로지스틱 회귀를 이용해서 분류를 모델을 만들어보자.

## 2-1. 로지스틱 회귀로 이진 분류 수행하기

### Step1. 데이터 준비하기

불리언 인덱싱을 이용해서 도미와 빙어의 행만을 골라내자.

> **불리언 인덱싱(boolean indexing)**
넘파이 배열은 True, False 값을 전달하여 행을 선택할 수 있는데, 이를 **불리언 인덱싱(boolean indexing)**이라고 한다. 간단한 예를 살펴보자.


```python
# Example : boolean indexing 
char_arr = np.array(['A', 'B', 'C', 'D', 'E'])
print(char_arr[[True, False, True, False, False]])
```

    ['A' 'C']
    


```python
# extract Bream and Smelt data
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

###  Step2. 모델 훈련하기 

로지스틱 회귀 모델인 LogisticRegression 클래스는 선형 모델로 sklearn.linear_model 패키지 아래에 있다.


```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```




    LogisticRegression()



### 모델의 예측값

#### predict() 메서드 

predict() 메서드로 아래와 같이 예측한 샘플을 확인할 수 있다.


```python
# check some of predict 
print(lr.predict(train_bream_smelt[:5]))
```

    ['Bream' 'Smelt' 'Bream' 'Bream' 'Bream']
    

#### Predict_proba() 메서드

KNeighborsClassifier와 마찬가지로 예측 확률은 predict_proba() 메서드에서 제공한다. 위에서 예측한 샘플들의 확률를 살펴보자.


```python
# check Probability value
print(lr.predict_proba(train_bream_smelt[:5]))
```

    [[0.99759855 0.00240145]
     [0.02735183 0.97264817]
     [0.99486072 0.00513928]
     [0.98584202 0.01415798]
     [0.99767269 0.00232731]]
    

첫 번째 열은 음성 클래스(0)에 대한 확률이고 두 번째 열은 양성 클래스(1)에 대한 확률이다.  k-최근접 이웃 분류기에서 본 것처럼 사이킷런은 타깃값을 알파벳순으로 정렬해서 사용한다. 따라서, 첫 번째 열은 'Bream'에 대한 예측 확률, 두 번째 열은 'Smelt'에 대한 예측 확률이다. 

#### classes_ 속성

클래스의 순서는 classes_ 속성을 이용해서 확인할 수도 있다.


```python
print(lr.classes_)
```

    ['Bream' 'Smelt']
    

* 만약 Bream을 양성 클래스로 이용하려면 Bream 타깃값을 1로 만들고 나머지 타깃값은 0으로 만들어 사용하면 된다.

#### coef_, intercept 속성

로지스틱 회귀도 선형 방정식을 학습하므로 로지스틱 회귀가 학습한 계수를 확인할 수 있는데, 여기서도 마찬가지로 coef_, intercept_ 속성으로 확인할 수 있다.


```python
print(lr.coef_, lr.intercept_)
```

    [[-0.4037798  -0.57620209 -0.66280298 -1.01290277 -0.73168947]] [-2.16155132]
    

위의 정보를 통해 로지스틱 회귀 모델이 학습한 방정식은 다음의 식이다.

$z = -0.4037798 \times (Weight)  -0.57620209 \times (Length) -0.66280298 \times (Diagonal) -1.01290277 \times (Height) -0.73168947\times (Width)  -2.16155132$

### 예측값 직접 계산하기

#### decision_fucntion() 메서드 

LogisticRegression 모델로 z값을 계산한 결과는 decision_function() 메서드로 출력할 수 있다. 처음 5개의 샘플의 z값을 출력해보자.


```python
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
```

    [-6.02927744  3.57123907 -5.26568906 -4.24321775 -6.0607117 ]
    

#### expit() 메서드

로지스틱 회귀는 선형 방정식을 통해 출력한 z값들을 시그모이드 함수에 통과시켜 확률값을 얻는다. 이를 직접 계산하기 위해서 scipy라이브러리에 시그모이드 함수를 이용해서 확률로 변환해보자.

* np.exp() 함수를 이용할 수 있으나 라이브러리 이용이 훨씬 편리하고 안전하다.


```python
from scipy.special import expit
print(expit(decisions))
```

    [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731]
    

위에서 얻은 확률 값은 앞서 predict_proba()메서드로 확인한 양성 클래스의 확률값과 일치함을 알 수 있다. 즉, decision_function()메서드는 양성 클래스의 z값을 반환한다.

> **요약 : 로지스틱 회귀를 이용한 이진 분류**
> 1. predict() : 예측한 클래스 출력
> 2. predict_proba() : 음성 클래스 | 양성 클래스의 확률 출력
> 3. coef_ 속성, intercept_ 속성 : 로지스틱 모델이 학습한 선형 방정식의 계수
> 4. decision_function() : 양성 클래스에 대한 z값 출력
> 5. expit() : 시그모이드 함수


## 2-2. 로지스틱 회귀로 다중 분류 수행하기

이제 LogisticRegression 클래스를 이용해서 7가지 종류의 생선을 분류해보자.

### 소개

LogisticRegression 클래스는 기본적으로 반복적인 알고리즘을 사용한다. 반복 횟수를 지정하는 매개변수로 max_iter가 있고 기본값은 100이다. 

또 LogisticRegression은 기본적으로 릿지 회귀와 같이 계수의 제곱을 규제한다. 릿지 회귀에서는 alpha 값을 조절해서 규제를 조절하지만, LogisticRegression은 C 매개변수를 통해 규제를 조절한다. 기본값은 1이고 릿지 회귀와 반대로 값이 커지면 규제의 정도가 약해지고 값이 작아지면 규제의 정도가 강해진다.

데이터는 수정할 필요없이 이진 분류에서 사용한 데이터를 그대로 이용해도 괜찮다. 따라서, 바로 모델을 훈련해보자. 

### Step1. 모델 훈련


```python
# train model
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
```




    LogisticRegression(C=20, max_iter=1000)



### Step2. 모델 평가


```python
# evaluation
print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

    0.9327731092436975
    0.925
    

훈련 세트와 테스트 세트에 대한 점수도 높고 과대적합이나 과소적합이 나타난 것 같지는 않다. 좋은 결과가 나온 것을 확인할 수 있다.

### 모델의 예측값

#### predict() 메서드

이번에도 predict() 메서드로 예측값들을 몇 가지 확인해보자. 


```python
# check prediction sample
print(lr.predict(test_scaled[:5]))
```

    ['Perch' 'Smelt' 'Pike' 'Roach' 'Perch']
    

#### predict_proba() 메서드

predict_proba()를 이용해서 위의 예측값에 대한 확률도 출력해보자.


```python
# check probability values of prediction sample
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    

이진 분류와의 차이점은 각 샘플에 대해서 7개의 예측값이 나왔다는 것이다. 

#### classes_ 속성

각각의 열이 어떤 생선을 나타내는지 classes_속성에서 클래스 정보를 확인해보자.


```python
print(lr.classes_)
```

    ['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish']
    

로지스틱 회귀 모델이 다중 분류에서는 어떤 선형 방정식을 만들었는지 shape를 이용해서 확인해보자.


```python
print(lr.coef_.shape, lr.intercept_.shape)
```

    (7, 5) (7,)
    

5개의 특성을 이용하므로 coef_ 배열의 열은 5개이다. 그런데, coef_ 배열에 행과 intercept_의 행이 7개가 있다는 것은 z를 7개나 계산한다는 의미이다.이는 곧 다중 분류는 클래스마다 z값을 하나씩 계산하고 z값을 출력하는 클래스가 예측 클래스가 된다는 의미이다. 즉, 한가지 샘플이 들어오면 7번의 이진 분류로 각각의 확률을 계산하고 그 중 가장 큰 확률인 것이 예측 클래스가 된다.

### 예측값 직접 계산하기

이진 분류에서처럼 predict_proba()로 나온 확률값들이 어떻게 나오는지 직접 계산해보자.

#### decision_function() 

각각의 z값들을 decision_function()으로 구해준다.


```python
# z value
decision = lr.decision_function(test_scaled[:5])
print(np.round(decision, decimals=2))
```

    [[ -6.5    1.03   5.16  -2.73   3.34   0.33  -0.63]
     [-10.86   1.93   4.77  -2.4    2.98   7.84  -4.26]
     [ -4.34  -6.23   3.17   6.49   2.36   2.42  -3.87]
     [ -0.68   0.45   2.65  -1.19   3.26  -5.75   1.26]
     [ -6.4   -1.99   5.82  -0.11   3.5   -0.11  -0.71]]
    

#### softmax()

이제 z값들을 소프트맥스 함수에 넣어서 확률값들을 얻어보자. 여기서 주의할 점은 softmax() 함수의 매개변수로 axis=1을 지정해주지 않으면 배열 전체에 대한 소프트맥스를 계산한다는 것이다.


```python
# softmax functino
from scipy.special import softmax
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
```

    [[0.    0.014 0.841 0.    0.136 0.007 0.003]
     [0.    0.003 0.044 0.    0.007 0.946 0.   ]
     [0.    0.    0.034 0.935 0.015 0.016 0.   ]
     [0.011 0.034 0.306 0.007 0.567 0.    0.076]
     [0.    0.    0.904 0.002 0.089 0.002 0.001]]
    
