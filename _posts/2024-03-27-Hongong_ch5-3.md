# Chapter 5-3 트리의 앙상블 - 실습

## 0. 소개

이번 글에서는 정형 데이터에서 좋은 성능을 내는 앙상블 학습 알고리즘을 직접 구현해본다. 사이킷런으로 구현해 볼 알고리즘은 다음과 같다.

1. 랜덤 포레스트
2. 엑스트라 트리
3. 그레이디언트 부스팅
4. 히스토그램 기반 그레이디언트 부스팅


## 1. 랜덤 포레스트

사이킷런에서 제공하는 랜덤 포레스트는 기본적으로 100개의 결정 트리를 훈련하며 분류 모델일 때는 RandomForestClassifier, 회귀 모델일 때는 RandomForestRegressor 클래스를 사용한다.

분류 모델인 RandomForestClassifier 클래스는 기본적으로 전체 특성의 개수의 제곱근만큼의 특성을 선택하고 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼는다. 회귀 모델인 RandomForestRegressor 클래스는 기본적으로 전체 특성을 사용하고 단순히 각 트리의 예측을 평균해서 예측값으로 삼는다.

RandomForestClasifier 클래스를 이용해서 화이트 와인을 분류하는 문제를 해결해보자.

### Step1. 데이터 불러오기 및 분할


```python
# import library
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# load data 
wine = pd.read_csv('https://bit.ly/wine_csv_data')

# split data to train/test and input/target
data = wine[['alcohol','sugar','pH']].to_numpy()
target = wine['class'].to_numpy()
train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)
```

### Step2. 모델 훈련 - 교차 검증

앞서 말했듯 사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 훈련하기 때문에 n_jobs 매개변수 값을 -1로 주어 CPU를 전부 사용하는 것이 좋다. 

교차 검증 함수인 cross_validate()함수를 이용할 때, returen_train_score 매개변수는 기본값이 False이지만 True로 지정하면 검증 점수뿐만 아니라 훈련 세트에 대한 점수도 같이 반환한다. 이를 이용하면 과대적합을 파악하는 데 용이하다.


```python
# import library
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

# train random forest model using cross validation
rf = RandomForestClassifier(n_jobs=None, random_state=42)
scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=None)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.9973541965122431 0.8905151032797809
    

출력 결과 과대적합이 되어있는 것을 볼 수 있다. 여기서는 알고리즘을 조사하는 것이 주 목적이므로 더 이상의 매개변수 조정은 하지 않는다.

* 사실 이 예제는 매우 간단하고 특성이 많지 않아 그리드 서치를 사용하더라도 하이퍼파라미터 튜닝의 결과가 크게 나아지지 않는다.

**OOB 샘플를 검증 세트로 이용하기**

RandomForestClassifier의 기능 중에는 OOB 샘플을 이용해서 자체적으로 모델을 평가하는 기능이 있다. 이를 이용해서 교차 검증을 대신할 수 있어 훈련 세트에 더 많은 샘플을 이용할 수 있다는 장점이 있다. 

이 점수를 얻으려면 RandomForestClassifier 클래스의 oob_score 매개변수를 True로 지정해야 한다.(기본값은 False이다.) 이렇게 하면 랜덤 포레스트는 각 결정 트리의 OOB 점수를 평균하여 출력한다. 


```python
# score using OOB sample
rf = RandomForestClassifier(oob_score=True, n_jobs=None, random_state=42)
rf.fit(train_input, train_target)
print(rf.oob_score_)
```

    0.8934000384837406
    

### Step3. 특성 중요도 

랜덤 포레스트는 결정 트리의 앙상블이기 때문에, DecisionTreeClassifier가 제공하는 중요한 매개변수를 모두 제공한다.

> 매개변수 종류
criterion, max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf 등등

또한, 결정 트리의 큰 장점인 특성 중요도도 계산해준다. 랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다.


```python
# feature importance
rf.fit(train_input, train_target)
print(rf.feature_importances_)
```

    [0.23167441 0.50039841 0.26792718]
    

랜덤하게 특성을 선택해서 노드를 분할하는 랜덤 포레스트의 특징으로 인해서 ch5-1에서 결정 트리로 모델을 훈련했을 때와는 다르게 두 번째 특성의 중요도가 감소하고 다른 특성의 중요도는 증가한 것을 확인할 수 있다.


## 2. 엑스트라 트리

사이킷런에서 제공하는 엑스트라 트리는 분류 모델일 때는 ExtraTreesClassifier, 회귀 모델일 때는 ExtraTreesRegressor 클래스이다. 랜덤 포레스트처럼 기본적으로 100개의 결정 트리를 훈련하며 랜덤 포레스트와 동일하게 결정 트리가 제공하는 대부분의 매개변수를 지원한다. 

* 엑스트라 트리는 DecisionTreeClassifier의 splitter 매개변수를 'random'으로 지정한 결정 트리와 같다. 

### Step1. 모델 훈련 - 교차 검증


```python
# import library
from sklearn.ensemble import ExtraTreesClassifier

# train extra tree model using cross calidation
et = ExtraTreesClassifier(n_jobs=None, random_state=42)
scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=None)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.9974503966084433 0.8887848893166506
    

결과 랜덤 포레스트와 비슷하게 나왔는데, 그 이유는 이 예제는 특성이 많지 않아 두 모델의 차이가 크지 않기 때문이다. 보통 엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트보다 더 많은 결정 트리를 훈련해야 하지만, 랜덤하게 노드를 분할해서 계산 속도가 빠르다.

* 결정 트리는 최적의 분할을 찾는 데 시간을 많이 소모한다.

### Step2. 특성 중요도

엑스트라 트리도 특성 중요도를 제공하며, 랜덤 포레스트와 동일하게 feature_importances_ 속성으로 확인할 수 있다. 


```python
# feature importance
et.fit(train_input,train_target)
print(et.feature_importances_)
```

    [0.20183568 0.52242907 0.27573525]
    

결과를 보면 랜덤 포레스트와 마찬가지로 당도에 대한 특성 중요도가 결정 트리 모델에 비해 감소한 것을 확인할 수 있다.

## 3. 그레이디언트 부스팅

사이킷런에서 제공하는 그레이디언트 부스팅은 분류 모델일 때는 GradientBoostingClassifier , 회귀 모델일 때는 GradientBoostingRegressor 클래스이다. 

분류 모델인 GradientBoostingClasifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다. 경사 하강법이 조금씩 업데이터하는 것처럼 깊이가 얕은 결정 트리로 업데이트를 해나간다.

### Step1. 모델 훈련 - 교차 검증


```python
# import library
from sklearn.ensemble import GradientBoostingClassifier

# train gradient boosting model using cross validation
gb = GradientBoostingClassifier(random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=None)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.8881086892152563 0.8720430147331015
    

결과를 확인해보면 과대적합이 거의 되지 않았음을 확인할 수 있다. 그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강하다.



### Step2. 하이퍼파라미터 튜닝

학습률을 증가시키고 트리의 개수를늘리면 조금 더 성능을 향상시킬 수 있다. learning_rate(학습률) 와 n_estimators(트리의 개수)의 매개변수 값을 조절해서 학습률을 높이고 결정 트리 개수를 500개로 늘려보자.

* 기본값은 learning_rate=0.1, n_estimators=100 이다.


```python
# hyperparameter tunning
gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)
scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=None)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.9464595437171814 0.8780082549788999
    

학습률은 2배로 트리 개수는 5배로 늘려도 과대적합을 잘 억제하고 있는 것을 확인할 수 있다. 

### Step 3. 특성 중요도

그레이디언트 부스팅도 특성 중요도를 제공하며, 앞서 사용한 feature_importances_ 속성으로 확인할 수 있다.


```python
# feature importance
gb.fit(train_input, train_target)
print(gb.feature_importances_)
```

    [0.15872278 0.68011572 0.16116151]
    

결과에서 확인할 수 있듯이 그레이디언트 부스팅이 랜덤 포레스트보다 일부 특성(여기서는 당도)에 더 집중하는 것을 볼 수 있다.

### Step4. 추가 특징

추가로, 그레이디언트 부스팅에는 subsample이라는 매개변수가 있다. 이 매개변수는 트리 훈련에 사용할 훈련 세트의 비율을 정하며, 1.0이 기본값으로 훈련 세트 전체를 사용한다. 1보다 작아지면 훈련 세트의 일부를 사용하는데, 이는 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택해 진행하는 확률적 경사 하강법이나 미니배치 경사 하강법과 비슷하다.

일반적으로 그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있다. 그러나, 순서대로 트리를 추가하기 때문에 훈련 속도가 느리고 GradientBoostingClassifer에는 n_jobs 매개변수가 없다.


## 4. 히스토그램 기반 그레이디언트 부스팅

사이킷런의 히스토그램 기반 그레이디언트 부스팅 클래스는 분류인 모델일 때 HistGradientBoostingClassifier, 회귀인 모델일때 HistGradientBoostingRegressor이다. 

HistGradientBoostingClassifier 클래스는 일반적으로 기본 매개변수에서 안정적인 성능을 얻을 수 있다. 트리 개수를 지정하는데, n_estimators 대신에 부스팅 반복 횟수를 지정하는 max_iter를 사용한다. 따라서, 성능을 높이려면 max_iter 매개변수를 테스트해 보아야 한다.

### Step1. 모델 훈련 - 교차 검증


```python
# import library
from sklearn.ensemble import HistGradientBoostingClassifier

# train histogram-based gradient boosting model using cross validation
hgb = HistGradientBoostingClassifier(random_state=42)
scores = cross_validate(hgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.9321723946453317 0.8801241948619236
    

과대적합을 잘 억제하면서 그레이디언트 부스팅보다 조금 더 높은 성능을 제공한다. 


### Step2. 특성 중요도


히스토그램 기반 그레이디언트 부스팅은 특성의 중요도를 계산하기 위해서는 permutation_importance() 함수를 사용한다. 이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지 관찰하여 어떤 특성이 중요한지 계산한다. 또한, 반환하는 객체는 반복하여 얻은 특성 중요도(importacnes)뿐만 아니라, 평균(importances_mean), 표준편차(importance_std)를 담고 있다. 

* n_repeats 매개변수는 랜덤하게 섞을 횟수를 지정하며, 기본값은 5이다.


```python
# import library
from sklearn.inspection import permutation_importance

# feature importance
hgb.fit(train_input, train_target)
result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=None)
print(result.importances_mean)
```

    [0.08876275 0.23438522 0.08027708]
    

permutation_importance() 함수는 훈련 세트, 테스트 세트, 사이킷런에서 제공하는 추정기 모델에도 적용할 수 있다. 다음은 테스트 세트에서 특성 중요도를 계산한 결과이다.


```python
# feature importance on test set
ruesult = permutation_importance(hgb, test_input, test_target, n_repeats=1, random_state=42, n_jobs=None)
print(result.importances_mean)
```

    [0.08876275 0.23438522 0.08027708]
    

테스트 세트에서의 결과를 확인해보면, 그레이디언트 부스팅과 비슷하게 당도에 집중하고 있다는 것을 알 수 있다. 

끝으로 테스트 세트에서 성능을 최종적으로 확인해보자.


```python
hgb.score(test_input, test_target)
```




    0.8723076923076923




히스토그램 기반 그레이디언트 부스팅 알고리즘은 사이킷런뿐만 아니라 다른 라이브러리에서도 제공한다. 여기서는 XGBoost와 LigtGBM 두 라이브러리를 간단하게 다룬다.

### XGBoost

XGBoost 라이브러리는 다양한 부스팅 알고리즘을 지원하며 사이킷런의 cross_validate() 함수와 함께 사용할 수 있다. 히스토그램 기반 그레이디언트 부스팅을 구현하기 위해서는 XGBClassifier 클래스를 통해 구현하며, tree_method 매개변수를 'hist'로 지정하면 사용할 수 있다.


```python
# import library
from xgboost import XGBClassifier

# train histogram-based gradient boosting model using cross validation
xgb = XGBClassifier(tree_method='hist', random_state=42)
scores = cross_validate(xgb, train_input, train_target, return_train_score=True)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    0.9555033709953124 0.8799326275264677
    

### LightGBM

LightGBM 라이브러리는 마이크로소프에서 만들었고 사이킷런의 히스토그램 기반 그레이디언트 부스팅이 많은 영향을 받은 라이브러리이다. LGBMClassifier 클래스를 통해서 히스토그램 기반 그레이디언트 부스팅을 구현할 수 있다.


```python
# import library
from lightgbm import LGBMClassifier

# train histogram-based gradient boosting model using cross validation
lgb = LGBMClassifier(random_state=42)
scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=None)
print(np.mean(scores['train_score']), np.mean(scores['test_score']))
```

    [LightGBM] [Info] Number of positive: 3151, number of negative: 1006
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000217 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 372
    [LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738
    [LightGBM] [Info] Start training from score 1.141738
    [LightGBM] [Info] Number of positive: 3151, number of negative: 1006
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000162 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 370
    [LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738
    [LightGBM] [Info] Start training from score 1.141738
    [LightGBM] [Info] Number of positive: 3151, number of negative: 1007
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000568 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 373
    [LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744
    [LightGBM] [Info] Start training from score 1.140744
    [LightGBM] [Info] Number of positive: 3151, number of negative: 1007
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000109 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 372
    [LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744
    [LightGBM] [Info] Start training from score 1.140744
    [LightGBM] [Info] Number of positive: 3152, number of negative: 1006
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000154 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 371
    [LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.758057 -> initscore=1.142055
    [LightGBM] [Info] Start training from score 1.142055
    0.935828414851749 0.8801251203079884
    
