---
layout: single
title: "혼공머신 ch4-2 - 실습"
use_math: true
---

# 4-2 확률적 경사 하강법 - 실습

## 0. 소개

사이킷런이 제공하는 확률적 경사 하강법 모델인 SGDClassifier 클래스를 이용해서 로지스틱 손실 함수를 사용한 분류 알고리즘을 만들어 본다나아가 에포크를 조절하면서 조기 종료 시기를 확인해서 적절한 하이퍼파라미터를 설정한다.


## 1.  SGDClassifier


### Step1. 데이터 준비하기


```python
# load data
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
```


```python
# split data to input and target
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```


```python
# split data to train and target
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)
```

확률적 경사 하강법은 가파른 경사를 파악해서 내려가야 하므로 표준화 작업으로 특성들을 똑같은 스케일로 만들어주는 작업이 필요하다. 주의할 점은 훈련 세트에서 학습한 통계 값으로 테스트 세트도 변환해야 한다는 것이다.


```python
# standardization
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

### Step2. 모델 훈련하기

사이킷런에서 확률적 경사 하강법을 제공하는 대표적인 분류용 클래스는 SGDClassifier이다. SGDClassifier의 객체를 만들 때 2개의 매개변수를 지정해야 한다. 

1. loss : 손실 함수의 종류를 지정
2. max_iter : 수행할 에포크를 지정

여기서는 loss='log'로 로지스틱 손실 함수를 지정하고 max_iter=10으로 전체 훈련 세트를 10번 반복한다.

* 다중 분류일 경우 SGDClassifier에 loss='log'로 지정하면 클래스마다 이진 분류 모델을 만든다. 이런 방식을 OvR(One versus Rest)라고 부른다.


```python
# train model
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
```

    C:\Users\data_study\anaconda3\envs\yonsei\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
      ConvergenceWarning,
    




    SGDClassifier(loss='log', max_iter=10, random_state=42)




```python
# evaluation
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.773109243697479
    0.775
    

### Step3. 모델 추가 훈련하기

모델의 점수가 높지 않게 나왔다. 에포크 횟수가 부족했을 수 있다. 확률적 경사 하강법은 점진적 학습이 가능하다. partial_fit() 메서드를 사용하면 SGDClassifier 객체를 다시 만들지 않고 만든 모델을 이어서 훈련할 수 있다. partial_fit() 메서드는 fit()메서드와 사용법이 같지만 호출할 때마다 1에포크씩 이어서 훈련한다.


```python
# additional training
sc.partial_fit(train_scaled,train_target)
```




    SGDClassifier(loss='log', max_iter=10, random_state=42)




```python
# evaluation
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

    0.8151260504201681
    0.85
    

SGDClassifier 객체에 train_scaled와 train_target을 한 번에 전달했지만, 이 알고리즘은 전달한 훈련 세트에서 1개씩 샘플을 꺼내어 경사 하강법 단계를 수행한다. 따라서, 배치 경사 하강법이 아닌 확률적 경사 하강법이다. 
* SGDClassifier는 미니배치 경사 하강법이나 배치 하강법을 제공하지 않는다.

### Step4. 조기 종료

확률적 경사 하강법은 적절한 에포크 횟수를 찾아서 조기 종료를 해야 모델이 과대적합이나 과소적합 되지 않는다. 적절한 에포크 횟수를 찾기 위해서 훈련하고 점수를 기록하면서 적절한 에포크 횟수를 찾아보자.

이번 예제에서는 fit() 메서드를 사용하지 않고 partial_fit() 메서드만 사용하기 때문에, 훈련 세트에 있는 전체 클래스의 레이블을 partial_fit() 메서드에 전달해 주어야 한다. 이를 위해서 np.unique() 함수로 타깃 데이터의 목록을 전달해주었다.


```python
# setting to find early stopping
import numpy as np
sc = SGDClassifier(loss='log', random_state=42)
train_score = []
test_score = []
classes = np.unique(train_target)
```


```python
# train model and record score
for _ in range(0,300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```


```python
# visualization
import matplotlib.pyplot as plt
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

에포크에 따른 훈련 세트와 테스트 세트 점수를 확인해 본 결과 약 100 에포크 이후부터 훈련 세트와 테스트 세트의 점수가 벌어지는 것을 볼 수 있다. 따라서, 100 에포크가 조기 종료의 적절한 지점이라고 볼 수 있다. 이를 이용해서 100 에포크로 최종 모델을 만들고 점수를 확인해보자.


```python
# train model with 100 epoch
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
```


```python
# evaluation
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

## 힌지 손실

이 세션을 마무리하기 전에 SGDClassifier의 loss 매개변수를 잠시 알아보자. loss 매개변수의 기본값은 'hinge'이다. **힌지 손실(hinge loss)**는 **서포트 벡터 머신(support vector machine)**이라 불리는 또 다른 머신러닝 알고리즘을 위한 손실 함수이다. 여기서 자세히 다루지는 않지만, 소프트 벡터 머신은 널리 사용하는 머신러닝 알고리즘 중 하나이며 SGDClassifier가 여러 종류의 손실 함수를 loss 매개변수에 지정하여 다양한 머신러닝 알고리즘을 지원한다는 것을 기억해두자.

아래는 힌지 손실을 이용한 모델을 간단하게 만든 것이다.


```python
# train model and evaluation
sc = SGDClassifier(loss='hinge', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```
