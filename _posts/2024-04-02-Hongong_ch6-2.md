# Chapter 6-2. k-평균 알고리즘 - 실습

## 0. 소개 

이번 글에서는 사이킷런에서 제공하는 KMeans 클래스를 통해 k-평균 알고리즘을 구현해보고 하이퍼파라미터를 조절해 좋은 성능을 내는 모델을 찾아본다. 또한, KMeans 클래스가 제공하는 다양한 메서드와 속성들을 통해서 작동 방식과 장점들을 알아본다.

## 1.KMeans 클래스

### Step1. 데이터 준비하기


```python
# import library
import wget
import numpy as np
import matplotlib.pyplot as plt

# download data
wget.download('https://bit.ly/fruits_300_data')
```

    100% [..........................................................................] 3000128 / 3000128




    'fruits_300_data'




```python
# load data
fruits = np.load('fruits_300_data')
```

k-평균 알고리즘 모델을 훈련하기 위해서 (샘플 개수, 너비, 높이) 크기의 3차원 배열을 (샘플 개수, 너비 $\times$ 높이) 크기를 가진 2차원 배열로 저장해야한다.


```python
# reshape data
fruits_2d = fruits.reshape(-1, 100*100)
```


### Step2. 모델 훈련

사이킷런의 k-평균 알고리즘은 sklearn.cluster 모듈 아래 KMeans 클래스에 구현되어 있다. 이 클래스에서 설정할 매개변수는 클러스터 개수를 지정하는 n_clusters이다. 여기서는 3가지 종류의 과일이므로 n_clusters=3으로 지정한다.

k-평균 알고리즘은 비지도 학습 모델이므로 타깃 데이터가 없다. 따라서, fit()메서드에 훈련 데이터만 입력해서 모델을 훈련한다.


```python
# import class
from sklearn.cluster import KMeans

# train model
km = KMeans(n_clusters=3, random_state=42)
km.fit(fruits_2d)
```




    KMeans(n_clusters=3, random_state=42)




### Step3. 결과 확인

#### labels_ 속성

군집된 결과는 KMean 클래스 객체의 labels_속성에 저장된다. labels_ 배열의 길이는 샘플 개수와 같다. 이 배열은 각 샘플이 어떤 레이블에 해당되는지 나타낸다. n_clusters=3으로 지정했기 때문에 labels_ 배열의 값은 0,1,2 중 하나이다. 


```python
# result of cluster
print(km.labels_)
```

    [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
     2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2
     2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
     0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
     1 1 1 1]
    

#### 이미지 확인하기

레이블값 0,1,2와 레이블 순서에는 어떤 의미도 없다. 실제 레이블 0,1,2가 어떤 과일 사진을 주로 모았는지 알아보려면 직접 이미지를 출력하는 것이다. 먼저, 레이블 0,1,2로 모은 샘플의 개수를 확인해보면 다음과 같다.


```python
# number of samples for each label
print(np.unique(km.labels_, return_counts=True))
```

    (array([0, 1, 2]), array([111,  98,  91], dtype=int64))
    

각각의 레이블에 모인 샘플 수가 100개 내외로 나왔다. 이제 직접 확인하기 위해서 그림을 출력하는 유틸리티 함수를 만들자.

draw_fruits() 함수는 (샘플 개수, 너비, 높이)의 3차원 배열을 입력받아 가로 10개씩 이미지를 출력한다. 샘플 개수에 따라 행과 열의 개수를 계산하고 figsize를 지정한다. figsize는 ratio 매개변수에 비례하여 커지며 기본값은 1이다. 

그다음 2중 for 반복문을 사용해 먼저 첫 번째 행을 따라 이미지를 그리고 다음 행을 따라 이미지를 그리는 식으로 계속된다.


```python
# function to display multiple images
def draw_fruits(arr, ratio=1):
    n = len(arr) # number of samples
    rows = int(np.ceil(n/10)) # displays 10 images per line.
    cols = n if rows < 2 else 10 # if there is only one row, then the number of colunms is equal to the number of samples. otherwise it is 10.
    fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n: # draw only up to n
                axs[i,j].imshow(arr[i*10 + j], cmap='gray_r')
            axs[i,j].axis('off')
    plt.show()
```

레이블이 0인 과일 사진을 그리기 위해서 km.labels_==0와 같이 쓰면 km.labels_ 배열에서 값이 0인 위치는 True, 그 외는 모두 Flase가 되는 넘파이의 불리언 인덱싱을 이용하면 된다.레이블이 0, 1, 2인 과일 사진을 차례대로 그리면 다음과 같다.


```python
draw_fruits(fruits[km.labels_==0])
```


    
![png](output_14_0.png)
    



```python
draw_fruits(fruits[km.labels_==1])
```


    
![png](output_15_0.png)
    



```python
draw_fruits(fruits[km.labels_==2])
```


    
![png](output_16_0.png)
    


결과를 확인해보면 완벼하게는 구분해내지는 못했지만, 훈련 데이터에 타깃 레이블을 전혀 제공하지 않았음에도 스스로 비슷한 샘플들을 꽤 잘 모은 것을 확인할 수 있다.

## 2. 클러스터 중심

### cluster_centers_ 속성

KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluster_centes_ 속성에 저장되어 있다. 이 배열은 fruits_2d 샘플의 클러스터 중심이기 때문에 이미지로 출력하려면 $100 \times 100$ 크기의 2차원 배열로 바꿔주어야 한다.


```python
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)
```


    
![png](output_18_0.png)
    


ch6-1에서 과일별 평균 이미지를 출력한것과 매우 비슷한 이미지를 확인할 수 잇다.

### transform() 메서드

KMeans 클래스는 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 transform () 메서드를 가지고 있다.transform() 메서드는 fit()메서드와 마찬가지로 2차원 배열을 입력받는다. 

* transform() 메서드가 있다는 것은 StandardScaler 클래스처럼 특성값을 변환하는 도구로 사용할 수 있다는 의미이다. 


인덱스 100인 샘플에 적용해보자. 2차원 배열을 입력받기 때문에, 1개의 샘플에 적용할 때 슬라이싱 연산자를 이용해야 한다.


```python
# transform() method
print(km.transform(fruits_2d[100:101]))
```

    [[3393.8136117  8837.37750892 5267.70439881]]
    

결과값의 의미는 각각 레이블 0,1,2의 클러스터 중심까지의 거리를 의미한다. 레이블 0인 첫 번째 클러스터까지의 거리가 가장 작으므로 레이블 0으로 예측을 했을 것이다.

### predict() 메서드

KMeans 클래스는 가장 가까운 클러스터 중심을 예측 클래스로 출력하는 predict() 메서드를 제공한다. 인덱스 100인 샘플을 어떻게 예측하는지 확인해보자.


```python
# prediction for sample with index 100
print(km.predict(fruits_2d[100:101]))
```

    [0]
    


```python
# image from sample with index 100
draw_fruits(fruits[100:101])
```


    
![png](output_23_0.png)
    


위에서 레이블이 0일 때 파인애플 사진을 군집했던 것을 확인했다. 인덱스 100인 과일 사진은 파인애플이므로 잘 예측한 것을 확인할 수 있다. 이제, 가장 가까운 거리에 있는 클러스터 중심을 샘플의 예측값으로 사용할 수 있다. 즉, 클러스터 중심을 특성 공학처럼 사용해서 데이터 셋의 차원을 10000차원에서 3차원으로 줄일 수 있다.

### n_iter_ 속성

k-평균 알고리즘은 반복적으로 클러스터 중심을 옮기면서 최적의 클러스터를 찾는다. 이때 알고리즘이 반복한 횟수는 KMeans 클래스의 n_iter_ 속성에 저장된다.


```python
# number of iterations 
print(km.n_iter_)
```

    4
    




## 3. 최적의 k 찾기

k-평균 알고리즘의 단점 중 하나는 클러스터 개수를 사전에 지정해야 한다는 것이다. 알고 있는 데이터 셋을 이용했기 때문에 n_clusters=3으로 지정해서 실습을 진행했지만, 실전에서는 몇 개의 클러스터가 있는지 알 수 없다. 

군집 알고리즘에서 적절한 k 값을 찾기 위한 완벽한 방법은 없다. 몇 가지 도구가 있지만 저마다 장단점이 있다. 여기서는 절절한 클러스터 개수를 찾기 위한 대표적인 방법인 **엘보우(elbow)** 방법에 대해서 알아본다.

### 엘보우 방법

k-평균 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있다. 이 거리의 제곱 합을 **이너셔(inertia)**라고 부른다. 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여 있는지를 나타내는 값으로 생각할 수 있다. 일반적으로 클러스터 개수가 늘어나면 클러스터 개개의 크기는 줄어들기 때문에 이너셔도 줄어든다. 엘보우 방법은 클러스터 개수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터 개수를 찾는 방법이다.

클러스터의 개수를 증가시키면서 이너셔를 그래프로 그리면 감소하는 속도가 꺾이는 지점이 있다. 이 지점부터는 클러스터 개수를 늘려도 클러스터에 잘 밀집된 정도가 크게 개선되지 않느다. 이 지점이 마치 팔꿈치 모양과 비슷해 엘보우 방법이라고 부른다.

![ch6-2-1.jpg](ch6-2-1.jpg)


### inertia_ 속성

KMeans 클래스는 자동으로 이너셔를 계산해서 inetia_ 속성으로 제공한다. 과일 데이터 셋으로 직접 엘보우 방법을 사용하기 위해서 클러스터 개수 k를 2~6까지 바꿔가며 KMeans 클래스를 5번 훈련하고 저장된 이너셔값을 그래프로 출력하면 다음과 같다.



```python
# list for recording inertia
inertia = []

# train model and append inertia
for k in range(2,7):
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(fruits_2d)
    inertia.append(km.inertia_)

# draw graph
plt.plot(range(2,7), inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
```


    
![png](output_27_0.png)
    


이 그래프에서는 꺾이는 지점이 뚜렷하지 않지만, k=3에서 기울기가 조금 꺾이는 것을 볼 수 있다. 엘보우 지점보다 개수가 많아지면 이너셔의 변화가 줄어들면서 군집의 효과도 줄어든다. 하지만 이 그래프에서는 이런 지점이 명확하지는 않는다.

<br><br>

# 출처 

혼자 공부하는 머신러닝+딥러닝 -박해선 지음-
[[혼자 공부하는 머신러닝+딥러닝] 15강. k-평균 알고리즘 작동 방식을 이해하고 비지도 학습 모델 만들기](https://youtu.be/SBdy0nSctRM?si=vdfyPePgVgL7ja96)


```python

```
